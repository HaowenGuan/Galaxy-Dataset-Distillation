# Dataset Distillation by Self-Adaptive Trajectory Matching

![gz2_1ipc_aug_noise](docs/gz2_1ipc_aug_noise.png)

This repository contains code for Self-Adaptive Trajectory Matching (STM) algorithm, an improved version of Matching Training Trajectory (MTT). It also includes code for processing Galaxy Zoo 2 dataset for paper: Galaxy Dataset Distillation.

## Getting Started

Clone the repository and create a python environment with pytorch, at least one cuda supported GPU is required.

## Generating Expert Trajectories

Before doing any distillation, you'll need to generate some expert trajectories using `buffer.py`. The following command will train 10 ConvNet models on CIFAR-10 with ZCA whitening for 50 epochs each:

```bash
python buffer.py --dataset=CIFAR10 --model=ConvNet --train_epochs=50 --num_experts=10 --zca --buffer_path={path_to_buffer_storage} --data_path={path_to_dataset}
```

Note that experts need only be trained once and can be re-used for multiple distillation experiments. The example trains only 10 experts for fast demo purpose, `--num_experts=10`. To get our reported results, we train 100 experts for each dataset.

## Available Datasets

* CIFAR10
* CIFAR100
* ImageNet
* GZoo2 (See Detail Below)
* GZoo_aug (See Detail Below)

### GZoo2

[Galaxy Zoo 2 (GZ2)](https://academic.oup.com/mnras/article/435/4/2835/1022913) is a survey based dataset. For better performance, we simplify the classification tree and build a curated version Based on [GZ2 Table 5](https://data.galaxyzoo.org), which contains 243500 images. The simplified classification tree is shown below.

![Classification Tree](docs/gz2_tree_9_class.png)

We sort the confidence of galaxies in descending order and pick the top 600 confident images for each class to form a curated dataset. The curated dataset is divided into 500 training and 100 testing images per class, result of total 4500 training image and 900 testing images. The curated dataset is named as GZoo2.

[Download our dataset](https://drive.google.com/drive/folders/1Ax4hj-EwnASp2qOyH5Io5WQS5lL0lNJG?usp=sharing) and save it in `{path_to_dataset}`.

### GZoo2_aug

GZoo2_aug is a dataset generated by applying rotational augmentation on GZoo2. Every training image in GZoo2 is rotated for 36 degree 10 times. GZoo2_aug contains 45000 training images and 900 testing images.

[Download our dataset](https://drive.google.com/drive/folders/1Ax4hj-EwnASp2qOyH5Io5WQS5lL0lNJG?usp=sharing) and save it in `{path_to_dataset}`.

## Distillation Hyperparameter

| Dataset   | Img/Cls | Synthetic Steps <br />$(N)$ | Maximum <br />Stage Iteration <br />$(T_{max})$ | Threshold <br />$(\sigma)$ | Learning Rate <br />(Pixels) | Learning Rate <br />(Step Size) | Initial Step Size <br />$(\alpha)$ |
| --------- | ------- | --------------------------- | ----------------------------------------------- | -------------------------- | ---------------------------- | ------------------------------- | ---------------------------------- |
|           | 1       | 50                          | 1000                                            | 5                          | 1000                         | 0.01                            | 0.01                               |
| CIFAR-10  | 10      | 30                          | 1000                                            | 5                          | 1000                         | 0.01                            | 0.01                               |
|           | 50      | 30                          | 1000                                            | 5                          | 1000                         | 0.01                            | 0.01                               |
|           | 1       | 20                          | 1000                                            | 5                          | 1000                         | 0.01                            | 0.01                               |
| CIFAR-100 | 10      | 20                          | 1000                                            | 5                          | 1000                         | 0.01                            | 0.01                               |
|           | 50      | -                           | -                                               | -                          | -                            | -                               | -                                  |
|           | 1       | 50                          | 1000                                            | 5                          | 10000                         | 0.01                            | 0.0001                             |
| GZoo2     | 10      | 20                          | 1000                                            | 5                          | 10000                        | 0.01                            | 0.0001                             |
|           | 50      | -                           | -                                               | -                          | -                            | -                               | -                                  |


### Distillation Documentation

| Argument Example                    | Description                                                                                                 |
| ----------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| `--init_epoch=1`                    | Stage Distillation starting epoch range, (recommending value: 1): `[0, init_epoch)`                         |
| `--lr_teacher 0.001 --init_epoch=1` | `Synthetic lr`, its length should match the value of `init_epoch`, split the values by space: `list[float]` |
| `--dataset=CIFAR10`                 | Dataset Name: `str`                                                                                         |
| `--ipc=1`                           | Distillation Image Per Class: `int`                                                                         |
| `--syn_steps=15`                    | Synthetic Step: `int`, usually the larger, the better, uses more GPU memory and slower.                     |
| `--expert_epochs=1`                 | Refer to MTT-Distillation. For this algorithm, Fix it to 1                                                  |
| `--max_start_epoch=29`              | Maximum epoch for stage distillation: `int`. Use the number of epoch in buffer minus 1.                     |
| `--lr_img=1000`                     | Learning rate for updating synthetic image: `int`                                                           |
| `--lr_lr=0.01`                      | Learning rate for updating lr_teacher: `float`                                                              |
| `--pix_init=noise`                  | Synthetic image initialization. Choose from: {noise/real}                                                   |
| `--buffer_path={path}`              | Buffer path: `str`                                                                                          |
| `--data_path={path_to_dataset}`     | Dataset Path: `str`                                                                                         |
| `--Iteration=10000`                 | Maximum number of iteration: `int`                                                                          |
| `--eval_it=200`                     | Evaluation interval: `int`                                                                                  |
| `--prev_iter=0`                     | (Optional: `default: 0`) Count the iteration from this given value.                                         |
| `--wandb_name=Job_1`                | (Optional: `str`) Customize your wandb job name                                                             |
| `--load_syn_image={path}`           | (Optional: `str`) Load pretrained synthetic image, format is `wandb_name/images_#.pt`                       |

## WanDB Guide

Stagewise-MTT algorithm generates an independent `loss` and `test_loss` for each individual starting epoch. When the values log into WanDB server, each value will be plotted in an individual plot. In this guide, we will show you how to plot them in a same plot for better visualization. For example:

![WanDB](docs/wandb_example.png)

* Delete all individual plot in the format of `Grand_Loss_epoch_` and `Next_Epoch_Loss_`. (The data is still saved in the backend of WanDB)
* Click edit panel, search `Grand_Loss_epoch_` or `Next_Epoch_Loss_` in y attribute, and manually add all same type data into the same plot.
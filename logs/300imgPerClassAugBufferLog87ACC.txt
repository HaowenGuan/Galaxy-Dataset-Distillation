ssh://sbcaesar@dais10.uwb.edu:22/data/sbcaesar/xuan_venv/bin/python3 -u /data/sbcaesar/mac_galaxy/buffer.py
Hyper-parameters:
 {'dataset': 'gzoo2', 'subset': 'imagenette', 'model': 'ConvNet', 'num_experts': 10, 'lr_teacher': 0.001, 'batch_train': 256, 'batch_real': 256, 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'buffer_path': '/data/sbcaesar/galaxy_buffers', 'train_epochs': 30, 'zca': False, 'decay': False, 'mom': 0, 'l2': 0, 'save_interval': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fbdc84b3520>}
BUILDING DATASET
100%|█████████████████████████████████| 48000/48000 [00:00<00:00, 105202.69it/s]
48000it [00:00, 3871590.78it/s]
class c = 0: 4800 real images
class c = 1: 4800 real images
class c = 2: 4800 real images
class c = 3: 4800 real images
class c = 4: 4800 real images
class c = 5: 4800 real images
class c = 6: 4800 real images
class c = 7: 4800 real images
class c = 8: 4800 real images
class c = 9: 4800 real images
real images channel 0, mean = 0.0001, std = 0.9997
real images channel 1, mean = 0.0002, std = 1.0002
real images channel 2, mean = -0.0002, std = 1.0004
Add weight to loss function tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
DC augmentation parameters:
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 0	Epoch: 0	Train Acc: 0.49264583333333334	Test Acc: 0.6366666666666667	AVG Train loss: 1.5476235548655193	AVG Test loss: 1.132256011168162
Itr: 0	Epoch: 1	Train Acc: 0.593125	Test Acc: 0.6783333333333333	AVG Train loss: 1.2035184542338053	AVG Test loss: 0.9637080025672913
Itr: 0	Epoch: 2	Train Acc: 0.6380625	Test Acc: 0.665	AVG Train loss: 1.06159308052063	AVG Test loss: 0.9073924930890401
Itr: 0	Epoch: 3	Train Acc: 0.66275	Test Acc: 0.7516666666666667	AVG Train loss: 0.9776180534362793	AVG Test loss: 0.7780415368080139
Itr: 0	Epoch: 4	Train Acc: 0.6771666666666667	Test Acc: 0.6683333333333333	AVG Train loss: 0.9309334427515665	AVG Test loss: 0.8742420848210652
Itr: 0	Epoch: 5	Train Acc: 0.6958333333333333	Test Acc: 0.7516666666666667	AVG Train loss: 0.8740417976379394	AVG Test loss: 0.7354618986447652
Itr: 0	Epoch: 6	Train Acc: 0.705625	Test Acc: 0.7716666666666666	AVG Train loss: 0.8447923444112142	AVG Test loss: 0.6851595465342204
Itr: 0	Epoch: 7	Train Acc: 0.7208958333333333	Test Acc: 0.795	AVG Train loss: 0.8074663950602213	AVG Test loss: 0.6554193429152171
Itr: 0	Epoch: 8	Train Acc: 0.726625	Test Acc: 0.76	AVG Train loss: 0.7850540537834167	AVG Test loss: 0.7047807749112447
Itr: 0	Epoch: 9	Train Acc: 0.7484583333333333	Test Acc: 0.7733333333333333	AVG Train loss: 0.7297392725944519	AVG Test loss: 0.6558447591463725
Itr: 0	Epoch: 10	Train Acc: 0.758375	Test Acc: 0.805	AVG Train loss: 0.7028161211013794	AVG Test loss: 0.6041281978289287
Itr: 0	Epoch: 11	Train Acc: 0.7535833333333334	Test Acc: 0.7516666666666667	AVG Train loss: 0.7111089331309001	AVG Test loss: 0.7137906885147095
Itr: 0	Epoch: 12	Train Acc: 0.7614791666666667	Test Acc: 0.8166666666666667	AVG Train loss: 0.6907549608548482	AVG Test loss: 0.5764727532863617
Itr: 0	Epoch: 13	Train Acc: 0.7625208333333333	Test Acc: 0.8033333333333333	AVG Train loss: 0.6885674365361532	AVG Test loss: 0.596965522368749
Itr: 0	Epoch: 14	Train Acc: 0.7725833333333333	Test Acc: 0.7183333333333334	AVG Train loss: 0.6617774836222331	AVG Test loss: 0.7128187457720438
Itr: 0	Epoch: 15	Train Acc: 0.7813125	Test Acc: 0.8133333333333334	AVG Train loss: 0.6389549161593119	AVG Test loss: 0.5585751895109813
Itr: 0	Epoch: 16	Train Acc: 0.7858333333333334	Test Acc: 0.825	AVG Train loss: 0.630458376566569	AVG Test loss: 0.5603547052542368
Itr: 0	Epoch: 17	Train Acc: 0.7937083333333333	Test Acc: 0.8233333333333334	AVG Train loss: 0.6068076231479644	AVG Test loss: 0.5285712766647339
Itr: 0	Epoch: 18	Train Acc: 0.7887916666666667	Test Acc: 0.8066666666666666	AVG Train loss: 0.6200995841026307	AVG Test loss: 0.5714981778462728
Itr: 0	Epoch: 19	Train Acc: 0.8045208333333334	Test Acc: 0.7783333333333333	AVG Train loss: 0.5760216670831044	AVG Test loss: 0.5793803215026856
Itr: 0	Epoch: 20	Train Acc: 0.8058958333333334	Test Acc: 0.8266666666666667	AVG Train loss: 0.5668034612337748	AVG Test loss: 0.4981736167271932
Itr: 0	Epoch: 21	Train Acc: 0.8075833333333333	Test Acc: 0.85	AVG Train loss: 0.5643218766848246	AVG Test loss: 0.4761142838001251
Itr: 0	Epoch: 22	Train Acc: 0.8106041666666667	Test Acc: 0.82	AVG Train loss: 0.5544449311892191	AVG Test loss: 0.4865515398979187
Itr: 0	Epoch: 23	Train Acc: 0.8175625	Test Acc: 0.8433333333333334	AVG Train loss: 0.5370659354527791	AVG Test loss: 0.4473803222179413
Itr: 0	Epoch: 24	Train Acc: 0.8214791666666666	Test Acc: 0.8316666666666667	AVG Train loss: 0.5213005295594533	AVG Test loss: 0.47697792887687684
Itr: 0	Epoch: 25	Train Acc: 0.8164583333333333	Test Acc: 0.815	AVG Train loss: 0.54209343957901	AVG Test loss: 0.5766459258397421
Itr: 0	Epoch: 26	Train Acc: 0.8217291666666666	Test Acc: 0.8316666666666667	AVG Train loss: 0.5283593149185181	AVG Test loss: 0.497036106189092
Itr: 0	Epoch: 27	Train Acc: 0.8270416666666667	Test Acc: 0.8183333333333334	AVG Train loss: 0.5125063761075338	AVG Test loss: 0.5030516560872396
Itr: 0	Epoch: 28	Train Acc: 0.8260833333333333	Test Acc: 0.8483333333333334	AVG Train loss: 0.508904921690623	AVG Test loss: 0.4371292539437612
Itr: 0	Epoch: 29	Train Acc: 0.8450833333333333	Test Acc: 0.8633333333333333	AVG Train loss: 0.4649704461892446	AVG Test loss: 0.431123065551122
train set ACC of each class tensor([0.0922, 0.0952, 0.0958, 0.0963, 0.0934, 0.0789, 0.0901, 0.0803, 0.0817,
        0.0979])
[[4425  188    0   13    0    0  141    9    4   20]
 [ 162 4571    8   23    0    0    8    8    1   19]
 [   0    7 4597  133   62    0    0    0    0    1]
 [   0    5   60 4624   71    2    1    4   23   10]
 [   0    0   50  231 4483    0    0    0    0   36]
 [   0    0    2   47    0 3788  699   75  169   20]
 [  64   14   21    0    0  110 4323    0  244   24]
 [  33    4   48   24    0  279  174 3854  342   42]
 [   5   16   20   83    8  113  513   95 3924   23]
 [  14   58    0   19    0    0    7    4    0 4698]]
test set ACC of each class tensor([0.0933, 0.0867, 0.0950, 0.0933, 0.0917, 0.0717, 0.0867, 0.0733, 0.0733,
        0.0983])
[[56  2  0  1  0  0  0  0  1  0]
 [ 4 52  0  0  0  0  2  1  0  1]
 [ 0  1 57  2  0  0  0  0  0  0]
 [ 0  0  2 56  2  0  0  0  0  0]
 [ 0  0  1  4 55  0  0  0  0  0]
 [ 0  0  1  1  0 43 10  2  3  0]
 [ 2  1  0  0  0  3 52  0  2  0]
 [ 0  0  0  1  1  6  5 44  3  0]
 [ 0  0  0  2  0  3  7  1 44  3]
 [ 0  0  1  0  0  0  0  0  0 59]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 1	Epoch: 0	Train Acc: 0.4865625	Test Acc: 0.6166666666666667	AVG Train loss: 1.563471315383911	AVG Test loss: 1.1161692396799723
Itr: 1	Epoch: 1	Train Acc: 0.6111666666666666	Test Acc: 0.7033333333333334	AVG Train loss: 1.1653645323117574	AVG Test loss: 0.9237756768862406
Itr: 1	Epoch: 2	Train Acc: 0.6404166666666666	Test Acc: 0.71	AVG Train loss: 1.0468224886258444	AVG Test loss: 0.8689883263905843
Itr: 1	Epoch: 3	Train Acc: 0.6587083333333333	Test Acc: 0.7416666666666667	AVG Train loss: 0.9841970040003458	AVG Test loss: 0.7816328064600626
Itr: 1	Epoch: 4	Train Acc: 0.682875	Test Acc: 0.7366666666666667	AVG Train loss: 0.916120970249176	AVG Test loss: 0.7644569500287374
Itr: 1	Epoch: 5	Train Acc: 0.69525	Test Acc: 0.745	AVG Train loss: 0.8846674432754517	AVG Test loss: 0.7190593242645263
Itr: 1	Epoch: 6	Train Acc: 0.7010833333333333	Test Acc: 0.7616666666666667	AVG Train loss: 0.8628863881429036	AVG Test loss: 0.7017415992418925
Itr: 1	Epoch: 7	Train Acc: 0.7305208333333333	Test Acc: 0.79	AVG Train loss: 0.7890780399640401	AVG Test loss: 0.6620314844449361
Itr: 1	Epoch: 8	Train Acc: 0.7342916666666667	Test Acc: 0.7933333333333333	AVG Train loss: 0.7682776783307393	AVG Test loss: 0.6194488175710042
Itr: 1	Epoch: 9	Train Acc: 0.744125	Test Acc: 0.77	AVG Train loss: 0.7499721872011821	AVG Test loss: 0.6463887127240499
Itr: 1	Epoch: 10	Train Acc: 0.7479583333333333	Test Acc: 0.7816666666666666	AVG Train loss: 0.7397654121716817	AVG Test loss: 0.663216579357783
Itr: 1	Epoch: 11	Train Acc: 0.7552708333333333	Test Acc: 0.7733333333333333	AVG Train loss: 0.7151610356966654	AVG Test loss: 0.6258498807748158
Itr: 1	Epoch: 12	Train Acc: 0.7644375	Test Acc: 0.785	AVG Train loss: 0.6923258380889893	AVG Test loss: 0.6001544018586477
Itr: 1	Epoch: 13	Train Acc: 0.773	Test Acc: 0.8166666666666667	AVG Train loss: 0.6651721830368043	AVG Test loss: 0.5615652513504028
Itr: 1	Epoch: 14	Train Acc: 0.770375	Test Acc: 0.8216666666666667	AVG Train loss: 0.6691845819155375	AVG Test loss: 0.5252003228664398
Itr: 1	Epoch: 15	Train Acc: 0.7737083333333333	Test Acc: 0.825	AVG Train loss: 0.6639115800857543	AVG Test loss: 0.5514906058708827
Itr: 1	Epoch: 16	Train Acc: 0.7819375	Test Acc: 0.8216666666666667	AVG Train loss: 0.6393325222333273	AVG Test loss: 0.5312756911913554
Itr: 1	Epoch: 17	Train Acc: 0.7976041666666667	Test Acc: 0.8133333333333334	AVG Train loss: 0.5953643034299214	AVG Test loss: 0.5271651927630107
Itr: 1	Epoch: 18	Train Acc: 0.7944791666666666	Test Acc: 0.835	AVG Train loss: 0.6063439981937409	AVG Test loss: 0.4956084903081258
Itr: 1	Epoch: 19	Train Acc: 0.80625	Test Acc: 0.8383333333333334	AVG Train loss: 0.5754782236417134	AVG Test loss: 0.4760386006037394
Itr: 1	Epoch: 20	Train Acc: 0.809875	Test Acc: 0.835	AVG Train loss: 0.5657861032485962	AVG Test loss: 0.48068720976511636
Itr: 1	Epoch: 21	Train Acc: 0.82125	Test Acc: 0.805	AVG Train loss: 0.5293878307342529	AVG Test loss: 0.5340455881754558
Itr: 1	Epoch: 22	Train Acc: 0.80725	Test Acc: 0.835	AVG Train loss: 0.5713005468050639	AVG Test loss: 0.46694170037905375
Itr: 1	Epoch: 23	Train Acc: 0.8186666666666667	Test Acc: 0.85	AVG Train loss: 0.5398390628496805	AVG Test loss: 0.4367587331930796
Itr: 1	Epoch: 24	Train Acc: 0.8195	Test Acc: 0.825	AVG Train loss: 0.5371904295285542	AVG Test loss: 0.5204959825674693
Itr: 1	Epoch: 25	Train Acc: 0.8259583333333333	Test Acc: 0.8733333333333333	AVG Train loss: 0.5126872989336649	AVG Test loss: 0.41543432195981345
Itr: 1	Epoch: 26	Train Acc: 0.8253333333333334	Test Acc: 0.8233333333333334	AVG Train loss: 0.5201908632914225	AVG Test loss: 0.48570242762565613
Itr: 1	Epoch: 27	Train Acc: 0.8355	Test Acc: 0.8516666666666667	AVG Train loss: 0.49265800340970356	AVG Test loss: 0.4195093842347463
Itr: 1	Epoch: 28	Train Acc: 0.8276458333333333	Test Acc: 0.875	AVG Train loss: 0.5096893429756164	AVG Test loss: 0.4049023616313934
Itr: 1	Epoch: 29	Train Acc: 0.8431875	Test Acc: 0.8633333333333333	AVG Train loss: 0.4684203689098358	AVG Test loss: 0.3875074191888173
train set ACC of each class tensor([0.0985, 0.0882, 0.0966, 0.0889, 0.0960, 0.0867, 0.0802, 0.0931, 0.0773,
        0.0976])
[[4726    1    0    0    0    0   23   30    0   20]
 [ 480 4233    0    0    0    0    0   76    1   10]
 [   0   37 4637   32   71    6    0    0    0   17]
 [   1   57   93 4268  225   21   37   43   38   17]
 [   0    0   54   87 4607    9    0    0    0   43]
 [   0    0    0    0    0 4162  249  329   40   20]
 [ 103    8    1    0    0  502 3848   49  265   24]
 [  21    0   10    2    0  124   26 4469  136   12]
 [  37    0   20    5    7  297  312  393 3712   17]
 [  35   44    0   14    0    0    0   20    1 4686]]
test set ACC of each class tensor([0.0967, 0.0850, 0.0933, 0.0883, 0.0900, 0.0800, 0.0767, 0.0883, 0.0667,
        0.0983])
[[58  1  0  0  0  0  0  0  1  0]
 [ 7 51  0  0  0  0  1  1  0  0]
 [ 0  2 56  1  0  0  1  0  0  0]
 [ 0  0  1 53  5  0  1  0  0  0]
 [ 0  0  0  4 54  2  0  0  0  0]
 [ 0  1  1  0  0 48  6  4  0  0]
 [ 2  1  0  0  0  6 46  3  2  0]
 [ 0  0  0  0  1  5  0 53  1  0]
 [ 0  0  0  1  0  6  6  4 40  3]
 [ 0  0  0  0  0  1  0  0  0 59]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 2	Epoch: 0	Train Acc: 0.5002708333333333	Test Acc: 0.6466666666666666	AVG Train loss: 1.5358450765609741	AVG Test loss: 1.1170987550417582
Itr: 2	Epoch: 1	Train Acc: 0.6155833333333334	Test Acc: 0.6566666666666666	AVG Train loss: 1.1559828227361044	AVG Test loss: 0.9787726298967997
Itr: 2	Epoch: 2	Train Acc: 0.6326041666666666	Test Acc: 0.7033333333333334	AVG Train loss: 1.0706331159273783	AVG Test loss: 0.8871074533462524
Itr: 2	Epoch: 3	Train Acc: 0.6618958333333333	Test Acc: 0.7583333333333333	AVG Train loss: 0.9788414454460144	AVG Test loss: 0.7816226347287496
Itr: 2	Epoch: 4	Train Acc: 0.6739791666666667	Test Acc: 0.7166666666666667	AVG Train loss: 0.9444469555219015	AVG Test loss: 0.7894187235832214
Itr: 2	Epoch: 5	Train Acc: 0.6927291666666666	Test Acc: 0.7316666666666667	AVG Train loss: 0.8856664319038391	AVG Test loss: 0.7627296312650045
Itr: 2	Epoch: 6	Train Acc: 0.716	Test Acc: 0.7633333333333333	AVG Train loss: 0.826775678952535	AVG Test loss: 0.6883396418889364
Itr: 2	Epoch: 7	Train Acc: 0.7055	Test Acc: 0.79	AVG Train loss: 0.8499808807373047	AVG Test loss: 0.6651205849647522
Itr: 2	Epoch: 8	Train Acc: 0.7239583333333334	Test Acc: 0.8183333333333334	AVG Train loss: 0.8016823898951213	AVG Test loss: 0.6195744907855988
Itr: 2	Epoch: 9	Train Acc: 0.7402708333333333	Test Acc: 0.7883333333333333	AVG Train loss: 0.7557351212501526	AVG Test loss: 0.6309734809398652
Itr: 2	Epoch: 10	Train Acc: 0.74525	Test Acc: 0.7983333333333333	AVG Train loss: 0.7460254252751668	AVG Test loss: 0.6001754927635193
Itr: 2	Epoch: 11	Train Acc: 0.76225	Test Acc: 0.7983333333333333	AVG Train loss: 0.6994351739883423	AVG Test loss: 0.6597457075119019
Itr: 2	Epoch: 12	Train Acc: 0.7626458333333334	Test Acc: 0.8066666666666666	AVG Train loss: 0.6987341470718383	AVG Test loss: 0.5932874353726705
Itr: 2	Epoch: 13	Train Acc: 0.773	Test Acc: 0.79	AVG Train loss: 0.6688369512557983	AVG Test loss: 0.6042566434542338
Itr: 2	Epoch: 14	Train Acc: 0.7693333333333333	Test Acc: 0.8466666666666667	AVG Train loss: 0.6756636367638906	AVG Test loss: 0.5119831168651581
Itr: 2	Epoch: 15	Train Acc: 0.7811875	Test Acc: 0.8216666666666667	AVG Train loss: 0.6509041164716085	AVG Test loss: 0.5495203606287639
Itr: 2	Epoch: 16	Train Acc: 0.7898125	Test Acc: 0.8483333333333334	AVG Train loss: 0.6236631585756938	AVG Test loss: 0.4916350797812144
Itr: 2	Epoch: 17	Train Acc: 0.7864791666666666	Test Acc: 0.8366666666666667	AVG Train loss: 0.6279090773264567	AVG Test loss: 0.5109683402379354
Itr: 2	Epoch: 18	Train Acc: 0.8044791666666666	Test Acc: 0.82	AVG Train loss: 0.576609751701355	AVG Test loss: 0.49512786904970807
Itr: 2	Epoch: 19	Train Acc: 0.7982916666666666	Test Acc: 0.8466666666666667	AVG Train loss: 0.5946574961344401	AVG Test loss: 0.4700808044274648
Itr: 2	Epoch: 20	Train Acc: 0.8065	Test Acc: 0.86	AVG Train loss: 0.5715828043619792	AVG Test loss: 0.46462734738985695
Itr: 2	Epoch: 21	Train Acc: 0.8082708333333334	Test Acc: 0.8466666666666667	AVG Train loss: 0.5744458065032959	AVG Test loss: 0.45706714431444806
Itr: 2	Epoch: 22	Train Acc: 0.8206666666666667	Test Acc: 0.8416666666666667	AVG Train loss: 0.5401804982821147	AVG Test loss: 0.4899172071615855
Itr: 2	Epoch: 23	Train Acc: 0.8139791666666667	Test Acc: 0.8516666666666667	AVG Train loss: 0.5527228748003642	AVG Test loss: 0.4477867654959361
Itr: 2	Epoch: 24	Train Acc: 0.8276875	Test Acc: 0.8683333333333333	AVG Train loss: 0.5169439401626587	AVG Test loss: 0.41648661176363627
Itr: 2	Epoch: 25	Train Acc: 0.8215416666666666	Test Acc: 0.8516666666666667	AVG Train loss: 0.5281185516516368	AVG Test loss: 0.4349407358964284
Itr: 2	Epoch: 26	Train Acc: 0.8131666666666667	Test Acc: 0.7933333333333333	AVG Train loss: 0.5499695499738058	AVG Test loss: 0.6453901839256286
Itr: 2	Epoch: 27	Train Acc: 0.8336458333333333	Test Acc: 0.865	AVG Train loss: 0.49559117420514426	AVG Test loss: 0.4087612521648407
Itr: 2	Epoch: 28	Train Acc: 0.8279166666666666	Test Acc: 0.865	AVG Train loss: 0.5084827133814493	AVG Test loss: 0.4027531333764394
Itr: 2	Epoch: 29	Train Acc: 0.83325	Test Acc: 0.8583333333333333	AVG Train loss: 0.4928782207965851	AVG Test loss: 0.4188222368558248
train set ACC of each class tensor([0.0981, 0.0914, 0.0978, 0.0903, 0.0942, 0.0801, 0.0908, 0.0912, 0.0588,
        0.0962])
[[4707    6    0    0    0    0   49   28    0   10]
 [ 380 4385    0    0    0    0    0   30    0    5]
 [   0   28 4693   20   45   11    2    0    0    1]
 [   0   46  119 4334  157   12   60   38   15   19]
 [   0    0   96  137 4520    5    0    0    0   42]
 [   1    5    1    0    0 3844  549  361   19   20]
 [  68   25   20    0    0  231 4358   23   62   13]
 [  51   23   39   16    0  123  140 4379   25    4]
 [  47   61   20   42    4  304 1093  395 2824   10]
 [  48   72    0   24    0    0   24   13    0 4619]]
test set ACC of each class tensor([0.0950, 0.0883, 0.0967, 0.0900, 0.0917, 0.0767, 0.0800, 0.0833, 0.0583,
        0.0983])
[[57  2  0  0  0  0  0  0  1  0]
 [ 5 53  0  0  0  0  1  1  0  0]
 [ 0  2 58  0  0  0  0  0  0  0]
 [ 0  0  3 54  3  0  0  0  0  0]
 [ 0  0  1  4 55  0  0  0  0  0]
 [ 0  0  1  1  0 46  9  3  0  0]
 [ 3  1  0  0  0  6 48  1  1  0]
 [ 1  0  1  1  0  2  5 50  0  0]
 [ 2  1  0  2  0  6  9  4 35  1]
 [ 0  0  0  0  0  1  0  0  0 59]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 3	Epoch: 0	Train Acc: 0.4841666666666667	Test Acc: 0.6266666666666667	AVG Train loss: 1.5547813577651977	AVG Test loss: 1.1514541506767273
Itr: 3	Epoch: 1	Train Acc: 0.58925	Test Acc: 0.6866666666666666	AVG Train loss: 1.211560118039449	AVG Test loss: 0.9558561412493388
Itr: 3	Epoch: 2	Train Acc: 0.6248541666666667	Test Acc: 0.705	AVG Train loss: 1.0921192051569621	AVG Test loss: 0.8749408062299092
Itr: 3	Epoch: 3	Train Acc: 0.6502291666666666	Test Acc: 0.7166666666666667	AVG Train loss: 1.0083359977404276	AVG Test loss: 0.85124405225118
Itr: 3	Epoch: 4	Train Acc: 0.6727916666666667	Test Acc: 0.725	AVG Train loss: 0.9414618918100993	AVG Test loss: 0.799950917561849
Itr: 3	Epoch: 5	Train Acc: 0.6804166666666667	Test Acc: 0.715	AVG Train loss: 0.9175697523752848	AVG Test loss: 0.7908775424957275
Itr: 3	Epoch: 6	Train Acc: 0.696875	Test Acc: 0.755	AVG Train loss: 0.8710390833218893	AVG Test loss: 0.7186120986938477
Itr: 3	Epoch: 7	Train Acc: 0.7184166666666667	Test Acc: 0.7616666666666667	AVG Train loss: 0.8179456295967102	AVG Test loss: 0.6931612396240234
Itr: 3	Epoch: 8	Train Acc: 0.7350208333333333	Test Acc: 0.775	AVG Train loss: 0.7678128765424093	AVG Test loss: 0.6569872156778971
Itr: 3	Epoch: 9	Train Acc: 0.7357083333333333	Test Acc: 0.7916666666666666	AVG Train loss: 0.7682103711764018	AVG Test loss: 0.6397365363438924
Itr: 3	Epoch: 10	Train Acc: 0.7387708333333334	Test Acc: 0.7783333333333333	AVG Train loss: 0.752860363483429	AVG Test loss: 0.6418213466803233
Itr: 3	Epoch: 11	Train Acc: 0.7607291666666667	Test Acc: 0.7766666666666666	AVG Train loss: 0.6978073854446412	AVG Test loss: 0.6180680831273396
Itr: 3	Epoch: 12	Train Acc: 0.7462083333333334	Test Acc: 0.78	AVG Train loss: 0.7366846591631572	AVG Test loss: 0.6179047743479411
Itr: 3	Epoch: 13	Train Acc: 0.7646875	Test Acc: 0.8116666666666666	AVG Train loss: 0.6792948559125265	AVG Test loss: 0.5727233767509461
Itr: 3	Epoch: 14	Train Acc: 0.7768541666666666	Test Acc: 0.7783333333333333	AVG Train loss: 0.6575151063601176	AVG Test loss: 0.592682797908783
Itr: 3	Epoch: 15	Train Acc: 0.7756458333333334	Test Acc: 0.8283333333333334	AVG Train loss: 0.656275858561198	AVG Test loss: 0.5458843322594961
Itr: 3	Epoch: 16	Train Acc: 0.7826666666666666	Test Acc: 0.8333333333333334	AVG Train loss: 0.6345326348940531	AVG Test loss: 0.5243024583657583
Itr: 3	Epoch: 17	Train Acc: 0.7836458333333334	Test Acc: 0.8066666666666666	AVG Train loss: 0.6314648342132568	AVG Test loss: 0.5652754004796346
Itr: 3	Epoch: 18	Train Acc: 0.7932916666666666	Test Acc: 0.7966666666666666	AVG Train loss: 0.6072483479181926	AVG Test loss: 0.5943002339204152
Itr: 3	Epoch: 19	Train Acc: 0.8001458333333333	Test Acc: 0.8366666666666667	AVG Train loss: 0.5882930469512939	AVG Test loss: 0.5369502047697703
Itr: 3	Epoch: 20	Train Acc: 0.8053125	Test Acc: 0.84	AVG Train loss: 0.5762323301633199	AVG Test loss: 0.4842102424303691
Itr: 3	Epoch: 21	Train Acc: 0.8154583333333333	Test Acc: 0.8183333333333334	AVG Train loss: 0.551956644932429	AVG Test loss: 0.5106791953245798
Itr: 3	Epoch: 22	Train Acc: 0.8196458333333333	Test Acc: 0.8283333333333334	AVG Train loss: 0.5386028499603271	AVG Test loss: 0.47345419724782306
Itr: 3	Epoch: 23	Train Acc: 0.8062916666666666	Test Acc: 0.8416666666666667	AVG Train loss: 0.5690682404836019	AVG Test loss: 0.46342533429463706
Itr: 3	Epoch: 24	Train Acc: 0.8139375	Test Acc: 0.8633333333333333	AVG Train loss: 0.5474817961851756	AVG Test loss: 0.4339983864625295
Itr: 3	Epoch: 25	Train Acc: 0.827125	Test Acc: 0.8566666666666667	AVG Train loss: 0.5122743906180064	AVG Test loss: 0.4358159323533376
Itr: 3	Epoch: 26	Train Acc: 0.8245833333333333	Test Acc: 0.855	AVG Train loss: 0.5192150352795919	AVG Test loss: 0.46729384978612265
Itr: 3	Epoch: 27	Train Acc: 0.8216666666666667	Test Acc: 0.875	AVG Train loss: 0.5311078397432963	AVG Test loss: 0.4127596898873647
Itr: 3	Epoch: 28	Train Acc: 0.8355208333333334	Test Acc: 0.8616666666666667	AVG Train loss: 0.48857677181561787	AVG Test loss: 0.41514837980270386
Itr: 3	Epoch: 29	Train Acc: 0.841	Test Acc: 0.8483333333333334	AVG Train loss: 0.47107150236765544	AVG Test loss: 0.46275454680124917
train set ACC of each class tensor([0.0956, 0.0856, 0.0848, 0.0950, 0.0904, 0.0731, 0.0870, 0.0926, 0.0815,
        0.0975])
[[4590    8    0    0    0    0  138   37    5   22]
 [ 522 4108    0    7    0    0   41   79   17   26]
 [   0  190 4072  425   42   34   21    3    1   12]
 [   1   24    3 4561   56    0   30   51   59   15]
 [   0    0   33  345 4341    0    0    0   19   62]
 [   0    0    0    0    0 3511  530  558  192    9]
 [  70    4    0    0    0  148 4176   68  324   10]
 [  17    2    0   17    0   56   74 4444  180   10]
 [   4    0    1   12    0  117  429  319 3911    7]
 [  19   29    0   22    0    0   16   20   12 4682]]
test set ACC of each class tensor([0.0950, 0.0833, 0.0800, 0.0950, 0.0883, 0.0650, 0.0800, 0.0883, 0.0733,
        0.1000])
[[57  2  0  0  0  0  0  0  1  0]
 [ 5 50  0  0  0  0  2  2  0  1]
 [ 1  2 48  7  0  0  0  0  2  0]
 [ 0  0  1 57  1  0  0  0  1  0]
 [ 0  0  0  5 53  2  0  0  0  0]
 [ 0  0  1  1  0 39  7  8  4  0]
 [ 2  1  0  0  0  2 48  5  2  0]
 [ 0  0  0  1  1  1  3 53  1  0]
 [ 0  0  0  2  0  5  5  4 44  0]
 [ 0  0  0  0  0  0  0  0  0 60]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 4	Epoch: 0	Train Acc: 0.5016041666666666	Test Acc: 0.64	AVG Train loss: 1.54457493241628	AVG Test loss: 1.1233189622561137
Itr: 4	Epoch: 1	Train Acc: 0.6061875	Test Acc: 0.7066666666666667	AVG Train loss: 1.1899288778305053	AVG Test loss: 0.927776845296224
Itr: 4	Epoch: 2	Train Acc: 0.6389791666666667	Test Acc: 0.6883333333333334	AVG Train loss: 1.067818619887034	AVG Test loss: 0.9477197265625
Itr: 4	Epoch: 3	Train Acc: 0.6591041666666667	Test Acc: 0.7216666666666667	AVG Train loss: 0.9959907484054565	AVG Test loss: 0.7885666521390279
Itr: 4	Epoch: 4	Train Acc: 0.683125	Test Acc: 0.7233333333333334	AVG Train loss: 0.9151465938886006	AVG Test loss: 0.7865867018699646
Itr: 4	Epoch: 5	Train Acc: 0.6907916666666667	Test Acc: 0.73	AVG Train loss: 0.8846325524648031	AVG Test loss: 0.759732944170634
Itr: 4	Epoch: 6	Train Acc: 0.7156666666666667	Test Acc: 0.755	AVG Train loss: 0.8258436857859294	AVG Test loss: 0.7134782417615255
Itr: 4	Epoch: 7	Train Acc: 0.7278958333333333	Test Acc: 0.7933333333333333	AVG Train loss: 0.7916282512346904	AVG Test loss: 0.6451217985153198
Itr: 4	Epoch: 8	Train Acc: 0.733625	Test Acc: 0.7766666666666666	AVG Train loss: 0.7736616746584575	AVG Test loss: 0.6378255399068197
Itr: 4	Epoch: 9	Train Acc: 0.7393541666666666	Test Acc: 0.7733333333333333	AVG Train loss: 0.7561576053301493	AVG Test loss: 0.642792387008667
Itr: 4	Epoch: 10	Train Acc: 0.76075	Test Acc: 0.7816666666666666	AVG Train loss: 0.699374074459076	AVG Test loss: 0.6634336511294047
Itr: 4	Epoch: 11	Train Acc: 0.7488125	Test Acc: 0.7983333333333333	AVG Train loss: 0.7313421689669292	AVG Test loss: 0.56942413965861
Itr: 4	Epoch: 12	Train Acc: 0.7574583333333333	Test Acc: 0.7933333333333333	AVG Train loss: 0.7060538125038147	AVG Test loss: 0.5942666403452556
Itr: 4	Epoch: 13	Train Acc: 0.7635	Test Acc: 0.8216666666666667	AVG Train loss: 0.6891154330571493	AVG Test loss: 0.5414913439750672
Itr: 4	Epoch: 14	Train Acc: 0.7899166666666667	Test Acc: 0.8316666666666667	AVG Train loss: 0.6204199275970459	AVG Test loss: 0.5121608591079712
Itr: 4	Epoch: 15	Train Acc: 0.7849791666666667	Test Acc: 0.83	AVG Train loss: 0.6332581187089285	AVG Test loss: 0.5099488333861033
Itr: 4	Epoch: 16	Train Acc: 0.7851875	Test Acc: 0.84	AVG Train loss: 0.6333113822937012	AVG Test loss: 0.4941697140534719
Itr: 4	Epoch: 17	Train Acc: 0.8023125	Test Acc: 0.83	AVG Train loss: 0.5874984340667725	AVG Test loss: 0.4844003729025523
Itr: 4	Epoch: 18	Train Acc: 0.7967083333333334	Test Acc: 0.8533333333333334	AVG Train loss: 0.5987394483884175	AVG Test loss: 0.4746082413196564
Itr: 4	Epoch: 19	Train Acc: 0.7938125	Test Acc: 0.8166666666666667	AVG Train loss: 0.6020658950805664	AVG Test loss: 0.526813721259435
Itr: 4	Epoch: 20	Train Acc: 0.8038541666666666	Test Acc: 0.8583333333333333	AVG Train loss: 0.5765345210234324	AVG Test loss: 0.4531269021828969
Itr: 4	Epoch: 21	Train Acc: 0.8168958333333334	Test Acc: 0.8466666666666667	AVG Train loss: 0.5445823531150817	AVG Test loss: 0.44682048678398134
Itr: 4	Epoch: 22	Train Acc: 0.8139166666666666	Test Acc: 0.86	AVG Train loss: 0.5490499043464661	AVG Test loss: 0.4413107204437256
Itr: 4	Epoch: 23	Train Acc: 0.823375	Test Acc: 0.8583333333333333	AVG Train loss: 0.5218019905885061	AVG Test loss: 0.4126809132099152
Itr: 4	Epoch: 24	Train Acc: 0.8177083333333334	Test Acc: 0.8533333333333334	AVG Train loss: 0.5383108127911885	AVG Test loss: 0.40790138363838196
Itr: 4	Epoch: 25	Train Acc: 0.8304791666666667	Test Acc: 0.8416666666666667	AVG Train loss: 0.5068304813702901	AVG Test loss: 0.4444697483380636
Itr: 4	Epoch: 26	Train Acc: 0.8405208333333334	Test Acc: 0.8583333333333333	AVG Train loss: 0.4825432612101237	AVG Test loss: 0.42238712747891743
Itr: 4	Epoch: 27	Train Acc: 0.8406875	Test Acc: 0.825	AVG Train loss: 0.47705494705835977	AVG Test loss: 0.43567253788312277
Itr: 4	Epoch: 28	Train Acc: 0.8357291666666666	Test Acc: 0.83	AVG Train loss: 0.49056944036483763	AVG Test loss: 0.46285691698392234
Itr: 4	Epoch: 29	Train Acc: 0.8538541666666667	Test Acc: 0.855	AVG Train loss: 0.43758769281705223	AVG Test loss: 0.42261005779107413
train set ACC of each class tensor([0.0864, 0.0924, 0.0945, 0.0901, 0.0978, 0.0901, 0.0745, 0.0774, 0.0933,
        0.0960])
[[4149  217    0   43    0    0  189   41  138   23]
 [ 106 4437   15   33    0    0   30   49  117   13]
 [   0    2 4538   74  184    2    0    0    0    0]
 [   0    0   28 4323  355    5   11    0   75    3]
 [   0    0    4   81 4696    0    0    0    0   19]
 [   0    0    0    0    0 4325  154   23  295    3]
 [  18    0   10    0    0  515 3578    4  675    0]
 [   2    0   12    6    3  404    2 3717  646    8]
 [   0    0   19    3   16  121   82   77 4479    3]
 [  11   47    0   38    5    0   15   20   58 4606]]
test set ACC of each class tensor([0.0817, 0.0917, 0.0950, 0.0850, 0.0967, 0.0833, 0.0683, 0.0650, 0.0917,
        0.0967])
[[49  4  0  2  0  0  2  0  3  0]
 [ 0 55  0  0  0  0  1  1  3  0]
 [ 0  2 57  1  0  0  0  0  0  0]
 [ 0  0  1 51  7  0  0  0  1  0]
 [ 0  0  1  1 58  0  0  0  0  0]
 [ 0  0  0  1  0 50  5  0  4  0]
 [ 1  0  0  1  0 10 41  0  7  0]
 [ 0  0  0  1  1 10  1 39  8  0]
 [ 0  0  0  0  0  3  1  1 55  0]
 [ 0  0  0  0  0  1  0  0  1 58]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 5	Epoch: 0	Train Acc: 0.489125	Test Acc: 0.6616666666666666	AVG Train loss: 1.5582516333262126	AVG Test loss: 1.1053540388743082
Itr: 5	Epoch: 1	Train Acc: 0.5935416666666666	Test Acc: 0.6683333333333333	AVG Train loss: 1.208394325574239	AVG Test loss: 1.0129772202173868
Itr: 5	Epoch: 2	Train Acc: 0.6380625	Test Acc: 0.7133333333333334	AVG Train loss: 1.0601749833424885	AVG Test loss: 0.8773924024899801
Itr: 5	Epoch: 3	Train Acc: 0.6574583333333334	Test Acc: 0.73	AVG Train loss: 0.993851183573405	AVG Test loss: 0.7963373363018036
Itr: 5	Epoch: 4	Train Acc: 0.6873333333333334	Test Acc: 0.7216666666666667	AVG Train loss: 0.9136799818674723	AVG Test loss: 0.7915743311246236
Itr: 5	Epoch: 5	Train Acc: 0.6984583333333333	Test Acc: 0.6416666666666667	AVG Train loss: 0.8797546315193177	AVG Test loss: 0.9700718967119852
Itr: 5	Epoch: 6	Train Acc: 0.7058958333333333	Test Acc: 0.7416666666666667	AVG Train loss: 0.859230084101359	AVG Test loss: 0.7036459374427796
Itr: 5	Epoch: 7	Train Acc: 0.714	Test Acc: 0.7433333333333333	AVG Train loss: 0.8262218840916952	AVG Test loss: 0.7154065879185995
Itr: 5	Epoch: 8	Train Acc: 0.7323333333333333	Test Acc: 0.7783333333333333	AVG Train loss: 0.7799337800343832	AVG Test loss: 0.6599118765195211
Itr: 5	Epoch: 9	Train Acc: 0.7491666666666666	Test Acc: 0.79	AVG Train loss: 0.7362465051015218	AVG Test loss: 0.6172989972432454
Itr: 5	Epoch: 10	Train Acc: 0.7454791666666667	Test Acc: 0.7816666666666666	AVG Train loss: 0.7426008230845134	AVG Test loss: 0.6678084508577983
Itr: 5	Epoch: 11	Train Acc: 0.7455833333333334	Test Acc: 0.7483333333333333	AVG Train loss: 0.740216474533081	AVG Test loss: 0.6731489570935567
Itr: 5	Epoch: 12	Train Acc: 0.7569166666666667	Test Acc: 0.7966666666666666	AVG Train loss: 0.712157351175944	AVG Test loss: 0.5903564814726512
Itr: 5	Epoch: 13	Train Acc: 0.7680833333333333	Test Acc: 0.8216666666666667	AVG Train loss: 0.6822369856834412	AVG Test loss: 0.5540972717603048
Itr: 5	Epoch: 14	Train Acc: 0.7796875	Test Acc: 0.78	AVG Train loss: 0.6460194081465404	AVG Test loss: 0.6088045259316762
Itr: 5	Epoch: 15	Train Acc: 0.7716458333333334	Test Acc: 0.8033333333333333	AVG Train loss: 0.6667513205210368	AVG Test loss: 0.5610729630788167
Itr: 5	Epoch: 16	Train Acc: 0.7892916666666666	Test Acc: 0.7866666666666666	AVG Train loss: 0.6235782895088195	AVG Test loss: 0.5775146190325419
Itr: 5	Epoch: 17	Train Acc: 0.7927708333333333	Test Acc: 0.8083333333333333	AVG Train loss: 0.6096371689637502	AVG Test loss: 0.5783280285199484
Itr: 5	Epoch: 18	Train Acc: 0.8029375	Test Acc: 0.8366666666666667	AVG Train loss: 0.5838405870596568	AVG Test loss: 0.4997003670533498
Itr: 5	Epoch: 19	Train Acc: 0.8041458333333333	Test Acc: 0.8133333333333334	AVG Train loss: 0.5794157999356587	AVG Test loss: 0.5113636994361878
Itr: 5	Epoch: 20	Train Acc: 0.8016666666666666	Test Acc: 0.84	AVG Train loss: 0.5805690495173136	AVG Test loss: 0.5048857613404591
Itr: 5	Epoch: 21	Train Acc: 0.8052708333333334	Test Acc: 0.8283333333333334	AVG Train loss: 0.5792153965632121	AVG Test loss: 0.4800646193822225
Itr: 5	Epoch: 22	Train Acc: 0.8109791666666667	Test Acc: 0.8483333333333334	AVG Train loss: 0.5551699611345927	AVG Test loss: 0.4631165436903636
Itr: 5	Epoch: 23	Train Acc: 0.8088333333333333	Test Acc: 0.8483333333333334	AVG Train loss: 0.5619756939411163	AVG Test loss: 0.4567292209466298
Itr: 5	Epoch: 24	Train Acc: 0.8224583333333333	Test Acc: 0.8283333333333334	AVG Train loss: 0.5238874391714732	AVG Test loss: 0.46322285175323485
Itr: 5	Epoch: 25	Train Acc: 0.8153541666666667	Test Acc: 0.8183333333333334	AVG Train loss: 0.5432627952098846	AVG Test loss: 0.4765729769070943
Itr: 5	Epoch: 26	Train Acc: 0.8420625	Test Acc: 0.8433333333333334	AVG Train loss: 0.47434202233950296	AVG Test loss: 0.41949247439702353
Itr: 5	Epoch: 27	Train Acc: 0.8289166666666666	Test Acc: 0.855	AVG Train loss: 0.5060869580109915	AVG Test loss: 0.4168519421418508
Itr: 5	Epoch: 28	Train Acc: 0.8330416666666667	Test Acc: 0.84	AVG Train loss: 0.4993021655877431	AVG Test loss: 0.4454221473137538
Itr: 5	Epoch: 29	Train Acc: 0.8408125	Test Acc: 0.8316666666666667	AVG Train loss: 0.4820844405492147	AVG Test loss: 0.43150996208190917
train set ACC of each class tensor([0.0976, 0.0932, 0.0978, 0.0873, 0.0961, 0.0832, 0.0857, 0.0925, 0.0595,
        0.0968])
[[4687   45    0    0    0    0   21   30    0   17]
 [ 285 4473    1    0    0    0    0   37    0    4]
 [   0   20 4695   20   62    3    0    0    0    0]
 [   0   46  186 4189  262    7   49   33    9   19]
 [   0    0   82   79 4614    2    0    0    0   23]
 [   4    6    0    0    0 3994  398  367   11   20]
 [ 128   48   20    2    0  367 4112   28   84   11]
 [  50    7   44    6    0  132   84 4442   29    6]
 [  60   63   30   37   20  419  837  457 2855   22]
 [  40   61    4   25    0    0    4   19    0 4647]]
test set ACC of each class tensor([0.0933, 0.0867, 0.0967, 0.0800, 0.0900, 0.0767, 0.0783, 0.0833, 0.0517,
        0.0950])
[[56  3  0  0  0  0  0  0  1  0]
 [ 6 52  0  0  0  0  1  1  0  0]
 [ 0  2 58  0  0  0  0  0  0  0]
 [ 0  1  4 48  7  0  0  0  0  0]
 [ 0  0  1  4 54  1  0  0  0  0]
 [ 0  0  2  0  0 46  8  4  0  0]
 [ 3  1  0  0  0  6 47  2  1  0]
 [ 2  0  1  0  0  6  1 50  0  0]
 [ 4  0  1  2  0  7  8  6 31  1]
 [ 0  1  1  0  0  0  0  0  1 57]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 6	Epoch: 0	Train Acc: 0.499625	Test Acc: 0.6483333333333333	AVG Train loss: 1.5396481459935507	AVG Test loss: 1.131027581691742
Itr: 6	Epoch: 1	Train Acc: 0.6029375	Test Acc: 0.7033333333333334	AVG Train loss: 1.1865579118728637	AVG Test loss: 0.9516249998410543
Itr: 6	Epoch: 2	Train Acc: 0.6290833333333333	Test Acc: 0.6816666666666666	AVG Train loss: 1.0852640172640482	AVG Test loss: 0.9079521918296813
Itr: 6	Epoch: 3	Train Acc: 0.6593541666666667	Test Acc: 0.7233333333333334	AVG Train loss: 0.9905543678601583	AVG Test loss: 0.8315083726247152
Itr: 6	Epoch: 4	Train Acc: 0.683625	Test Acc: 0.7233333333333334	AVG Train loss: 0.9179283488591512	AVG Test loss: 0.8174902931849162
Itr: 6	Epoch: 5	Train Acc: 0.6901041666666666	Test Acc: 0.74	AVG Train loss: 0.8914995061556498	AVG Test loss: 0.749041109085083
Itr: 6	Epoch: 6	Train Acc: 0.6995833333333333	Test Acc: 0.7816666666666666	AVG Train loss: 0.8697864391009013	AVG Test loss: 0.699285425345103
Itr: 6	Epoch: 7	Train Acc: 0.716	Test Acc: 0.7783333333333333	AVG Train loss: 0.8174066510200501	AVG Test loss: 0.6713198518753052
Itr: 6	Epoch: 8	Train Acc: 0.7311041666666667	Test Acc: 0.81	AVG Train loss: 0.7817276611328124	AVG Test loss: 0.6427198147773743
Itr: 6	Epoch: 9	Train Acc: 0.7397916666666666	Test Acc: 0.79	AVG Train loss: 0.7563635727564494	AVG Test loss: 0.6373327906926473
Itr: 6	Epoch: 10	Train Acc: 0.7391666666666666	Test Acc: 0.795	AVG Train loss: 0.7580722778638204	AVG Test loss: 0.6040124424298604
Itr: 6	Epoch: 11	Train Acc: 0.7555833333333334	Test Acc: 0.785	AVG Train loss: 0.7169457157452901	AVG Test loss: 0.6179226382573446
Itr: 6	Epoch: 12	Train Acc: 0.7569791666666666	Test Acc: 0.82	AVG Train loss: 0.710986230691274	AVG Test loss: 0.5803288519382477
Itr: 6	Epoch: 13	Train Acc: 0.7671458333333333	Test Acc: 0.81	AVG Train loss: 0.6783756686846415	AVG Test loss: 0.5601695132255554
Itr: 6	Epoch: 14	Train Acc: 0.7757291666666667	Test Acc: 0.82	AVG Train loss: 0.6587373765309652	AVG Test loss: 0.5338276751836141
Itr: 6	Epoch: 15	Train Acc: 0.77925	Test Acc: 0.83	AVG Train loss: 0.6485897076924642	AVG Test loss: 0.5275808290640513
Itr: 6	Epoch: 16	Train Acc: 0.78675	Test Acc: 0.83	AVG Train loss: 0.6219452687899272	AVG Test loss: 0.5232515319188435
Itr: 6	Epoch: 17	Train Acc: 0.7916666666666666	Test Acc: 0.82	AVG Train loss: 0.6170681325594584	AVG Test loss: 0.49959554235140485
Itr: 6	Epoch: 18	Train Acc: 0.7876666666666666	Test Acc: 0.7883333333333333	AVG Train loss: 0.6250516300201416	AVG Test loss: 0.5815661855538686
Itr: 6	Epoch: 19	Train Acc: 0.8041875	Test Acc: 0.775	AVG Train loss: 0.582765509446462	AVG Test loss: 0.5946822969118754
Itr: 6	Epoch: 20	Train Acc: 0.8011458333333333	Test Acc: 0.8216666666666667	AVG Train loss: 0.5891204020182291	AVG Test loss: 0.5226613318920136
Itr: 6	Epoch: 21	Train Acc: 0.8135833333333333	Test Acc: 0.835	AVG Train loss: 0.5526527121861776	AVG Test loss: 0.46093493620554604
Itr: 6	Epoch: 22	Train Acc: 0.8177291666666666	Test Acc: 0.8366666666666667	AVG Train loss: 0.5407811415990194	AVG Test loss: 0.4496250236034393
Itr: 6	Epoch: 23	Train Acc: 0.8172291666666667	Test Acc: 0.8066666666666666	AVG Train loss: 0.5440121046702067	AVG Test loss: 0.5353084103266398
Itr: 6	Epoch: 24	Train Acc: 0.8132291666666667	Test Acc: 0.825	AVG Train loss: 0.5519229084650675	AVG Test loss: 0.5210005430380503
Itr: 6	Epoch: 25	Train Acc: 0.829375	Test Acc: 0.84	AVG Train loss: 0.5067405667304993	AVG Test loss: 0.41687402566274007
Itr: 6	Epoch: 26	Train Acc: 0.8244166666666667	Test Acc: 0.8466666666666667	AVG Train loss: 0.5197011924584707	AVG Test loss: 0.4791622153917948
Itr: 6	Epoch: 27	Train Acc: 0.8318958333333333	Test Acc: 0.8066666666666666	AVG Train loss: 0.5071233175595602	AVG Test loss: 0.5457933859030406
Itr: 6	Epoch: 28	Train Acc: 0.8429791666666666	Test Acc: 0.7916666666666666	AVG Train loss: 0.4713277777036031	AVG Test loss: 0.5136378463109335
Itr: 6	Epoch: 29	Train Acc: 0.8341041666666666	Test Acc: 0.8483333333333334	AVG Train loss: 0.49022404607137043	AVG Test loss: 0.40589662750562033
train set ACC of each class tensor([0.0866, 0.0953, 0.0971, 0.0885, 0.0979, 0.0858, 0.0861, 0.0922, 0.0773,
        0.0971])
[[4156  344    0   16    0    0  171   78    5   30]
 [ 109 4573    7   17    0    0   19   56    1   18]
 [   0   13 4661   30   92    0    0    0    0    4]
 [   0   13  143 4247  309    6   19   36   25    2]
 [   0    0   23   56 4698    0    0    0    0   23]
 [   0    2    0    2    0 4119  372  236   49   20]
 [  31   13   21    0    0  343 4131   20  211   30]
 [   0    2   53   16    0  139   67 4425   90    8]
 [   0   12   20   28   16  268  436  297 3711   12]
 [  10   44    0   26    8    0   14   20   17 4661]]
test set ACC of each class tensor([0.0817, 0.0933, 0.0950, 0.0817, 0.0967, 0.0800, 0.0767, 0.0817, 0.0650,
        0.0967])
[[49  6  0  1  0  0  1  1  2  0]
 [ 0 56  0  0  0  0  2  2  0  0]
 [ 0  1 57  1  0  0  1  0  0  0]
 [ 0  0  4 49  7  0  0  0  0  0]
 [ 0  0  1  1 58  0  0  0  0  0]
 [ 0  0  1  1  0 48  8  2  0  0]
 [ 0  2  0  0  0  9 46  1  2  0]
 [ 0  0  0  0  1  7  2 49  1  0]
 [ 0  0  0  2  0  6  8  3 39  2]
 [ 0  0  1  0  0  0  0  0  1 58]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 7	Epoch: 0	Train Acc: 0.49454166666666666	Test Acc: 0.6616666666666666	AVG Train loss: 1.5468226188023886	AVG Test loss: 1.096117566426595
Itr: 7	Epoch: 1	Train Acc: 0.5884791666666667	Test Acc: 0.6433333333333333	AVG Train loss: 1.2105256762504577	AVG Test loss: 1.0164103595415752
Itr: 7	Epoch: 2	Train Acc: 0.6328333333333334	Test Acc: 0.7016666666666667	AVG Train loss: 1.0709020929336548	AVG Test loss: 0.8683708985646565
Itr: 7	Epoch: 3	Train Acc: 0.6645	Test Acc: 0.7333333333333333	AVG Train loss: 0.9727435647646586	AVG Test loss: 0.7717991860707601
Itr: 7	Epoch: 4	Train Acc: 0.6787708333333333	Test Acc: 0.7566666666666667	AVG Train loss: 0.9341433464686076	AVG Test loss: 0.7635894330342611
Itr: 7	Epoch: 5	Train Acc: 0.7065416666666666	Test Acc: 0.725	AVG Train loss: 0.8538905957539876	AVG Test loss: 0.7763439607620239
Itr: 7	Epoch: 6	Train Acc: 0.7052916666666667	Test Acc: 0.7816666666666666	AVG Train loss: 0.8538075132369995	AVG Test loss: 0.688062846660614
Itr: 7	Epoch: 7	Train Acc: 0.71575	Test Acc: 0.7816666666666666	AVG Train loss: 0.8232173191706339	AVG Test loss: 0.6848389999071757
Itr: 7	Epoch: 8	Train Acc: 0.7307916666666666	Test Acc: 0.775	AVG Train loss: 0.7832524994214376	AVG Test loss: 0.6356876571973165
Itr: 7	Epoch: 9	Train Acc: 0.7371458333333333	Test Acc: 0.8033333333333333	AVG Train loss: 0.763115474541982	AVG Test loss: 0.6142088011900584
Itr: 7	Epoch: 10	Train Acc: 0.7482708333333333	Test Acc: 0.815	AVG Train loss: 0.7375035397211711	AVG Test loss: 0.5866063503424327
Itr: 7	Epoch: 11	Train Acc: 0.7586458333333334	Test Acc: 0.81	AVG Train loss: 0.7111695834795634	AVG Test loss: 0.5767642533779145
Itr: 7	Epoch: 12	Train Acc: 0.7527708333333333	Test Acc: 0.8266666666666667	AVG Train loss: 0.7188265254497528	AVG Test loss: 0.5652150543530782
Itr: 7	Epoch: 13	Train Acc: 0.7678541666666666	Test Acc: 0.7966666666666666	AVG Train loss: 0.6807643534342448	AVG Test loss: 0.602195383310318
Itr: 7	Epoch: 14	Train Acc: 0.7698333333333334	Test Acc: 0.8	AVG Train loss: 0.6740449670155844	AVG Test loss: 0.6162573746840159
Itr: 7	Epoch: 15	Train Acc: 0.7746875	Test Acc: 0.8066666666666666	AVG Train loss: 0.6548016022046407	AVG Test loss: 0.5581157811482748
Itr: 7	Epoch: 16	Train Acc: 0.7831041666666667	Test Acc: 0.8266666666666667	AVG Train loss: 0.6336798714796702	AVG Test loss: 0.5310287809371949
Itr: 7	Epoch: 17	Train Acc: 0.7874791666666666	Test Acc: 0.8383333333333334	AVG Train loss: 0.6257386683622996	AVG Test loss: 0.5151202603181203
Itr: 7	Epoch: 18	Train Acc: 0.809375	Test Acc: 0.785	AVG Train loss: 0.5707841711044311	AVG Test loss: 0.5980193289120992
Itr: 7	Epoch: 19	Train Acc: 0.8036458333333333	Test Acc: 0.845	AVG Train loss: 0.5796677654584249	AVG Test loss: 0.48282634258270263
Itr: 7	Epoch: 20	Train Acc: 0.8041041666666666	Test Acc: 0.85	AVG Train loss: 0.5733231024742127	AVG Test loss: 0.4711094967524211
Itr: 7	Epoch: 21	Train Acc: 0.8084791666666666	Test Acc: 0.8416666666666667	AVG Train loss: 0.5707835459709167	AVG Test loss: 0.4656269184748332
Itr: 7	Epoch: 22	Train Acc: 0.8145625	Test Acc: 0.84	AVG Train loss: 0.5471187987327576	AVG Test loss: 0.4505410953362783
Itr: 7	Epoch: 23	Train Acc: 0.8156666666666667	Test Acc: 0.8433333333333334	AVG Train loss: 0.5429387812614441	AVG Test loss: 0.45380321900049847
Itr: 7	Epoch: 24	Train Acc: 0.8176458333333333	Test Acc: 0.8383333333333334	AVG Train loss: 0.5384058692455291	AVG Test loss: 0.48349708557128906
Itr: 7	Epoch: 25	Train Acc: 0.8220833333333334	Test Acc: 0.8483333333333334	AVG Train loss: 0.5265949824651083	AVG Test loss: 0.4341524302959442
Itr: 7	Epoch: 26	Train Acc: 0.8343541666666666	Test Acc: 0.8483333333333334	AVG Train loss: 0.4949647628466288	AVG Test loss: 0.4275510702530543
Itr: 7	Epoch: 27	Train Acc: 0.8276875	Test Acc: 0.855	AVG Train loss: 0.5130339645544688	AVG Test loss: 0.41873501261075335
Itr: 7	Epoch: 28	Train Acc: 0.8318958333333333	Test Acc: 0.8533333333333334	AVG Train loss: 0.5039537269274393	AVG Test loss: 0.45275821487108864
Itr: 7	Epoch: 29	Train Acc: 0.8404166666666667	Test Acc: 0.865	AVG Train loss: 0.47628523015975954	AVG Test loss: 0.4034111475944519
train set ACC of each class tensor([0.0898, 0.0947, 0.0967, 0.0845, 0.0982, 0.0849, 0.0745, 0.0857, 0.0915,
        0.0966])
[[4311  277    0    1    0    0   66   56   69   20]
 [ 132 4544    7   10    0    0    9   57   37    4]
 [   0   12 4643   21  124    0    0    0    0    0]
 [   0   28  148 4056  398    1    5   25  135    4]
 [   0    0   12   52 4715    0    0    0    0   21]
 [   0    2    0    0    0 4077  155  228  318   20]
 [  32   13   20    0    0  515 3576   17  619    8]
 [   0    0   18    0    0  170    1 4116  486    9]
 [   0    0   20    0    8  123  100  155 4391    3]
 [  21   49    3   24    8    0    8   20   29 4638]]
test set ACC of each class tensor([0.0867, 0.0917, 0.0967, 0.0800, 0.0983, 0.0833, 0.0617, 0.0833, 0.0867,
        0.0967])
[[52  4  0  0  0  0  0  2  2  0]
 [ 1 55  0  0  0  0  2  1  1  0]
 [ 0  1 58  1  0  0  0  0  0  0]
 [ 0  0  4 48  7  0  0  0  1  0]
 [ 0  0  1  0 59  0  0  0  0  0]
 [ 0  0  1  1  0 50  3  1  4  0]
 [ 0  2  0  0  0 12 37  0  9  0]
 [ 0  0  0  1  1  3  0 50  5  0]
 [ 0  0  0  0  0  3  3  1 52  1]
 [ 0  0  1  0  0  0  0  0  1 58]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 8	Epoch: 0	Train Acc: 0.48954166666666665	Test Acc: 0.5766666666666667	AVG Train loss: 1.5563371864954632	AVG Test loss: 1.2206716966629028
Itr: 8	Epoch: 1	Train Acc: 0.5931458333333334	Test Acc: 0.71	AVG Train loss: 1.2064314829508465	AVG Test loss: 0.9547955290476481
Itr: 8	Epoch: 2	Train Acc: 0.6215208333333333	Test Acc: 0.6866666666666666	AVG Train loss: 1.1001554306348165	AVG Test loss: 0.9027881137530009
Itr: 8	Epoch: 3	Train Acc: 0.6651875	Test Acc: 0.715	AVG Train loss: 0.9713522081375122	AVG Test loss: 0.8714007965723674
Itr: 8	Epoch: 4	Train Acc: 0.6716041666666667	Test Acc: 0.7233333333333334	AVG Train loss: 0.9459090557098389	AVG Test loss: 0.8049534249305725
Itr: 8	Epoch: 5	Train Acc: 0.6914166666666667	Test Acc: 0.7383333333333333	AVG Train loss: 0.8922796087265015	AVG Test loss: 0.7700554919242859
Itr: 8	Epoch: 6	Train Acc: 0.7040208333333333	Test Acc: 0.7566666666666667	AVG Train loss: 0.8546233019828796	AVG Test loss: 0.7168525465329488
Itr: 8	Epoch: 7	Train Acc: 0.6994375	Test Acc: 0.7683333333333333	AVG Train loss: 0.8619403861363729	AVG Test loss: 0.7312323506673177
Itr: 8	Epoch: 8	Train Acc: 0.7266458333333333	Test Acc: 0.7866666666666666	AVG Train loss: 0.7869985772768656	AVG Test loss: 0.6611443134148915
Itr: 8	Epoch: 9	Train Acc: 0.7271875	Test Acc: 0.7933333333333333	AVG Train loss: 0.7827297763824463	AVG Test loss: 0.6459867866834005
Itr: 8	Epoch: 10	Train Acc: 0.7327083333333333	Test Acc: 0.7983333333333333	AVG Train loss: 0.7708359837532044	AVG Test loss: 0.6263172682126363
Itr: 8	Epoch: 11	Train Acc: 0.7503958333333334	Test Acc: 0.795	AVG Train loss: 0.729256867090861	AVG Test loss: 0.607796524365743
Itr: 8	Epoch: 12	Train Acc: 0.7549375	Test Acc: 0.8066666666666666	AVG Train loss: 0.7106851166089376	AVG Test loss: 0.581329673131307
Itr: 8	Epoch: 13	Train Acc: 0.7632083333333334	Test Acc: 0.8083333333333333	AVG Train loss: 0.6856089517275492	AVG Test loss: 0.5791622916857402
Itr: 8	Epoch: 14	Train Acc: 0.760125	Test Acc: 0.815	AVG Train loss: 0.6971218479474386	AVG Test loss: 0.5728586379686992
Itr: 8	Epoch: 15	Train Acc: 0.78225	Test Acc: 0.805	AVG Train loss: 0.6352835413614909	AVG Test loss: 0.5899778846899668
Itr: 8	Epoch: 16	Train Acc: 0.7763125	Test Acc: 0.835	AVG Train loss: 0.6489556597073873	AVG Test loss: 0.5273202073574066
Itr: 8	Epoch: 17	Train Acc: 0.7893541666666667	Test Acc: 0.7916666666666666	AVG Train loss: 0.6132928593158722	AVG Test loss: 0.6439249761899313
Itr: 8	Epoch: 18	Train Acc: 0.788875	Test Acc: 0.8316666666666667	AVG Train loss: 0.6198777014414469	AVG Test loss: 0.5024367376168569
Itr: 8	Epoch: 19	Train Acc: 0.7921666666666667	Test Acc: 0.8	AVG Train loss: 0.6054728021621704	AVG Test loss: 0.5502302877108256
Itr: 8	Epoch: 20	Train Acc: 0.7945208333333333	Test Acc: 0.805	AVG Train loss: 0.5965029074350993	AVG Test loss: 0.570506143172582
Itr: 8	Epoch: 21	Train Acc: 0.8067083333333334	Test Acc: 0.8366666666666667	AVG Train loss: 0.5681722261110942	AVG Test loss: 0.5001844998200734
Itr: 8	Epoch: 22	Train Acc: 0.8136041666666667	Test Acc: 0.8383333333333334	AVG Train loss: 0.5525498323440552	AVG Test loss: 0.48794560114542646
Itr: 8	Epoch: 23	Train Acc: 0.8097083333333334	Test Acc: 0.8033333333333333	AVG Train loss: 0.5565216595331828	AVG Test loss: 0.6437485794226329
Itr: 8	Epoch: 24	Train Acc: 0.8155	Test Acc: 0.835	AVG Train loss: 0.5450502665837605	AVG Test loss: 0.47622990409533184
Itr: 8	Epoch: 25	Train Acc: 0.8155208333333334	Test Acc: 0.8283333333333334	AVG Train loss: 0.5447514017422994	AVG Test loss: 0.5277069306373596
Itr: 8	Epoch: 26	Train Acc: 0.8280208333333333	Test Acc: 0.8483333333333334	AVG Train loss: 0.5112904626528422	AVG Test loss: 0.46429584940274554
Itr: 8	Epoch: 27	Train Acc: 0.836625	Test Acc: 0.8483333333333334	AVG Train loss: 0.4890777629216512	AVG Test loss: 0.4284196456273397
Itr: 8	Epoch: 28	Train Acc: 0.8253958333333333	Test Acc: 0.8433333333333334	AVG Train loss: 0.5185173393885295	AVG Test loss: 0.47364781339963274
Itr: 8	Epoch: 29	Train Acc: 0.8233333333333334	Test Acc: 0.8633333333333333	AVG Train loss: 0.5192683905760447	AVG Test loss: 0.41083422700564065
train set ACC of each class tensor([0.0970, 0.0919, 0.0965, 0.0901, 0.0944, 0.0701, 0.0900, 0.0923, 0.0796,
        0.0964])
[[4657   31    0    0    0    0   62   34    0   16]
 [ 306 4411    0    2    0    0   10   59    3    9]
 [   0   48 4632   33   55    7   13    0    0   12]
 [   0   38  108 4325  170    4   44   48   61    2]
 [   0    0   75  166 4530    0    0    0    0   29]
 [   0    0    0    0    0 3364  668  561  190   17]
 [  62   10   12    2    0   86 4319   40  253   16]
 [  12    8   20    4    0   47  112 4430  159    8]
 [  28    0   20   13    0   99  509  300 3823    8]
 [  28   57    0   27    0    0   14   21   24 4629]]
test set ACC of each class tensor([0.0950, 0.0883, 0.0933, 0.0883, 0.0883, 0.0733, 0.0817, 0.0850, 0.0733,
        0.0967])
[[57  2  0  0  0  0  0  0  1  0]
 [ 4 53  0  0  0  0  2  1  0  0]
 [ 0  2 56  1  0  0  1  0  0  0]
 [ 0  0  3 53  4  0  0  0  0  0]
 [ 0  0  2  4 53  1  0  0  0  0]
 [ 0  0  1  1  0 44  8  3  3  0]
 [ 2  1  0  0  0  2 49  4  2  0]
 [ 0  0  1  1  0  1  4 51  2  0]
 [ 1  0  0  1  0  3  6  5 44  0]
 [ 0  0  1  0  0  0  0  1  0 58]]
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=32768, out_features=10, bias=True)
)
Itr: 9	Epoch: 0	Train Acc: 0.4839791666666667	Test Acc: 0.6133333333333333	AVG Train loss: 1.563596160888672	AVG Test loss: 1.1360956819852193
Itr: 9	Epoch: 1	Train Acc: 0.598375	Test Acc: 0.6916666666666667	AVG Train loss: 1.1916864444414774	AVG Test loss: 0.9557169580459595
Itr: 9	Epoch: 2	Train Acc: 0.6260625	Test Acc: 0.69	AVG Train loss: 1.0843136110305787	AVG Test loss: 0.8921717421213786
Itr: 9	Epoch: 3	Train Acc: 0.6594791666666666	Test Acc: 0.7116666666666667	AVG Train loss: 0.9832119522094727	AVG Test loss: 0.8518566727638245
Itr: 9	Epoch: 4	Train Acc: 0.6764583333333334	Test Acc: 0.7133333333333334	AVG Train loss: 0.9391665290196737	AVG Test loss: 0.8397115667661031
Itr: 9	Epoch: 5	Train Acc: 0.6802708333333334	Test Acc: 0.755	AVG Train loss: 0.9144313643773396	AVG Test loss: 0.7146264282862346
Itr: 9	Epoch: 6	Train Acc: 0.7109375	Test Acc: 0.7833333333333333	AVG Train loss: 0.8393596591949463	AVG Test loss: 0.6981544415156047
Itr: 9	Epoch: 7	Train Acc: 0.7222083333333333	Test Acc: 0.76	AVG Train loss: 0.8025065468152364	AVG Test loss: 0.7044329206148784
Itr: 9	Epoch: 8	Train Acc: 0.7191041666666667	Test Acc: 0.7	AVG Train loss: 0.8011261048316956	AVG Test loss: 0.8238979347546895
Itr: 9	Epoch: 9	Train Acc: 0.7392291666666667	Test Acc: 0.77	AVG Train loss: 0.7544170087178548	AVG Test loss: 0.6435887575149536
Itr: 9	Epoch: 10	Train Acc: 0.7471458333333333	Test Acc: 0.785	AVG Train loss: 0.7293202980359396	AVG Test loss: 0.639279670715332
Itr: 9	Epoch: 11	Train Acc: 0.7494375	Test Acc: 0.81	AVG Train loss: 0.726819678624471	AVG Test loss: 0.598220093647639
Itr: 9	Epoch: 12	Train Acc: 0.7713125	Test Acc: 0.8016666666666666	AVG Train loss: 0.6730689051946004	AVG Test loss: 0.602311075925827
Itr: 9	Epoch: 13	Train Acc: 0.7573125	Test Acc: 0.8	AVG Train loss: 0.7031126178105672	AVG Test loss: 0.5818743983904521
Itr: 9	Epoch: 14	Train Acc: 0.7673125	Test Acc: 0.8083333333333333	AVG Train loss: 0.6776706144014994	AVG Test loss: 0.5549503537019094
Itr: 9	Epoch: 15	Train Acc: 0.7718125	Test Acc: 0.815	AVG Train loss: 0.6624265777269999	AVG Test loss: 0.5336044192314148
Itr: 9	Epoch: 16	Train Acc: 0.7824583333333334	Test Acc: 0.8133333333333334	AVG Train loss: 0.637875216960907	AVG Test loss: 0.5463227673371633
Itr: 9	Epoch: 17	Train Acc: 0.7893541666666667	Test Acc: 0.81	AVG Train loss: 0.6153815457026164	AVG Test loss: 0.5591327198346456
Itr: 9	Epoch: 18	Train Acc: 0.7989791666666667	Test Acc: 0.835	AVG Train loss: 0.5955782407124838	AVG Test loss: 0.4916452876726786
Itr: 9	Epoch: 19	Train Acc: 0.7965208333333333	Test Acc: 0.8283333333333334	AVG Train loss: 0.5999043668111166	AVG Test loss: 0.49296044031778974
Itr: 9	Epoch: 20	Train Acc: 0.8067916666666667	Test Acc: 0.8366666666666667	AVG Train loss: 0.5737897500197092	AVG Test loss: 0.48844671885172525
Itr: 9	Epoch: 21	Train Acc: 0.811125	Test Acc: 0.83	AVG Train loss: 0.558274465084076	AVG Test loss: 0.4870473758379618
Itr: 9	Epoch: 22	Train Acc: 0.8031458333333333	Test Acc: 0.8433333333333334	AVG Train loss: 0.5777839795748393	AVG Test loss: 0.47788917541503906
Itr: 9	Epoch: 23	Train Acc: 0.8085833333333333	Test Acc: 0.8466666666666667	AVG Train loss: 0.564660591284434	AVG Test loss: 0.45175144950548807
Itr: 9	Epoch: 24	Train Acc: 0.82375	Test Acc: 0.8583333333333333	AVG Train loss: 0.5224386518796285	AVG Test loss: 0.4363660673300425
Itr: 9	Epoch: 25	Train Acc: 0.813875	Test Acc: 0.82	AVG Train loss: 0.5494531345367432	AVG Test loss: 0.5299082942803701
Itr: 9	Epoch: 26	Train Acc: 0.8251041666666666	Test Acc: 0.835	AVG Train loss: 0.5214384792645772	AVG Test loss: 0.47153565208117165
Itr: 9	Epoch: 27	Train Acc: 0.8331666666666667	Test Acc: 0.855	AVG Train loss: 0.49814328010876974	AVG Test loss: 0.4341700514157613
Itr: 9	Epoch: 28	Train Acc: 0.8417291666666666	Test Acc: 0.8583333333333333	AVG Train loss: 0.4786112058162689	AVG Test loss: 0.40981343269348147
Itr: 9	Epoch: 29	Train Acc: 0.827875	Test Acc: 0.85	AVG Train loss: 0.5051670311292012	AVG Test loss: 0.4320619986454646
train set ACC of each class tensor([0.0909, 0.0951, 0.0981, 0.0851, 0.0961, 0.0770, 0.0703, 0.0878, 0.0923,
        0.0974])
[[4363  257   16    4    0    0   44   34   48   34]
 [ 126 4563   22   17    0    0    1   24   14   33]
 [   0    7 4710   20   61    0    0    0    0    2]
 [   0   23  246 4085  304    1    1   23  100   17]
 [   0    0   70   85 4615    0    0    0    0   30]
 [   7    3    5    1    0 3694  189  411  470   20]
 [  66   52   24    0    0  370 3373   25  851   39]
 [  17    0   45    2    0   83    0 4215  421   17]
 [   0    2   27    0   12   82   77  157 4429   14]
 [  19   60    0   19    0    0    6   10   10 4676]]
test set ACC of each class tensor([0.0850, 0.0883, 0.0950, 0.0800, 0.0917, 0.0750, 0.0683, 0.0833, 0.0850,
        0.0983])
[[51  6  0  1  0  0  0  0  2  0]
 [ 2 53  0  0  0  0  0  1  3  1]
 [ 0  2 57  1  0  0  0  0  0  0]
 [ 0  0  6 48  5  0  0  0  1  0]
 [ 0  0  2  3 55  0  0  0  0  0]
 [ 0  0  1  1  0 45  4  3  6  0]
 [ 0  2  0  0  0  7 41  3  7  0]
 [ 0  0  1  1  0  1  0 50  7  0]
 [ 0  0  1  0  0  2  2  1 51  3]
 [ 0  0  1  0  0  0  0  0  0 59]]
Saving /data/sbcaesar/galaxy_buffers/gzoo2/ConvNet/replay_buffer_1.pt
/data/sbcaesar/mac_galaxy/buffer.py:172: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(12, 7))

Process finished with exit code 0

[[0.      0.14248 0.27646 0.40027 0.51924 0.63559 0.74952 0.8629  0.97554 1.08762 1.19924 1.31099 1.42289 1.53488 1.64633 1.75742 1.86878 1.98029 2.09185 2.20289 2.3149  2.42622 2.5367  2.64765 2.75894 2.86881 2.97931 3.08911 3.19844 3.30823]
 [0.      0.      0.03344 0.09661 0.17223 0.25516 0.34221 0.4326  0.52534 0.61961 0.71533 0.81267 0.91136 1.01102 1.11147 1.21219 1.31377 1.4163  1.5192  1.62202 1.7263  1.83    1.93356 2.03786 2.14261 2.24658 2.35102 2.45511 2.5591  2.6635 ]
 [0.      0.      0.      0.01942 0.06138 0.11722 0.18195 0.25313 0.32928 0.40879 0.4914  0.57687 0.66451 0.75415 0.84534 0.93746 1.03108 1.12601 1.22187 1.31805 1.41581 1.51352 1.61137 1.71022 1.80971 1.9087  2.00842 2.10791 2.20758 2.30761]
 [0.      0.      0.      0.      0.01407 0.04594 0.09037 0.14378 0.20426 0.26997 0.33995 0.41402 0.4913  0.57117 0.65348 0.73732 0.82327 0.91092 0.99989 1.08971 1.18118 1.27309 1.36552 1.45909 1.55354 1.64773 1.74283 1.83796 1.93345 2.02931]
 [0.      0.      0.      0.      0.      0.01133 0.03718 0.07421 0.12003 0.17255 0.23065 0.29375 0.36087 0.43143 0.50507 0.58084 0.65922 0.73972 0.82197 0.9055  0.99089 1.07716 1.16415 1.25257 1.34208 1.4316  1.52219 1.61301 1.70436 1.79626]
 [0.      0.      0.      0.      0.      0.      0.00956 0.03132 0.0634  0.10333 0.1499  0.20244 0.25968 0.32111 0.38618 0.45394 0.52496 0.59836 0.67393 0.75115 0.8305  0.91114 0.99278 1.07597 1.16053 1.24541 1.3315  1.41801 1.50515 1.59309]
 [0.      0.      0.      0.      0.      0.      0.      0.0083  0.02749 0.05552 0.09114 0.1334  0.18116 0.23376 0.29052 0.35053 0.41413 0.48057 0.54959 0.62061 0.69404 0.76913 0.84541 0.92347 1.00318 1.08333 1.16488 1.24718 1.33027 1.41418]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.00752 0.02432 0.04942 0.08179 0.12037 0.16429 0.21285 0.26515 0.32148 0.38107 0.44359 0.50843 0.5759  0.64538 0.71637 0.78935 0.86412 0.93965 1.01667 1.09476 1.17363 1.25361]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00668 0.0218  0.04465 0.07431 0.1098  0.15032 0.19514 0.24436 0.29713 0.35314 0.41193 0.47347 0.5374  0.60307 0.67098 0.74081 0.81169 0.88421 0.95796 1.03273 1.10869]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00614 0.02005 0.04111 0.06851 0.1013  0.13882 0.18109 0.22722 0.27686 0.32972 0.38549 0.44389 0.5043  0.56717 0.63214 0.6984  0.76653 0.83593 0.90661 0.97858]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00576 0.01867 0.03821 0.06356 0.09402 0.12947 0.16914 0.21258 0.25947 0.30959 0.36251 0.41773 0.47564 0.53586 0.59753 0.66115 0.72636 0.79289 0.86097]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00555 0.01749 0.03559 0.05907 0.08784 0.12113 0.15848 0.19947 0.244   0.29152 0.34163 0.39458 0.44997 0.50699 0.56627 0.62726 0.68968 0.75376]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00534 0.01639 0.03319 0.05551 0.08248 0.11389 0.1491  0.18808 0.23031 0.27531 0.32322 0.37396 0.4265  0.48132 0.53809 0.59642 0.65652]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00496 0.01532 0.03136 0.05233 0.07782 0.10746 0.14095 0.17787 0.21787 0.26103 0.30699 0.35492 0.40543 0.45796 0.51223 0.56842]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.0047  0.01455 0.02956 0.04935 0.07344 0.10157 0.13344 0.16846 0.20679 0.24807 0.29161 0.33778 0.38617 0.43642 0.48864]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00454 0.0138  0.02806 0.04684 0.06977 0.09652 0.1267  0.16033 0.19703 0.23622 0.2781  0.32233 0.36862 0.41702]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00432 0.01319 0.02671 0.04454 0.06637 0.09178 0.12076 0.15296 0.1878  0.22543 0.26564 0.30793 0.35247]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00415 0.01256 0.02547 0.04243 0.06312 0.08751 0.11531 0.14585 0.17932 0.21549 0.25391 0.29469]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00404 0.0121  0.02431 0.04045 0.06037 0.08377 0.11013 0.13952 0.17168 0.2062  0.24323]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00394 0.01161 0.02325 0.03874 0.05789 0.08006 0.10547 0.1337  0.16437 0.19771]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00375 0.01107 0.02226 0.03722 0.05536 0.07678 0.10111 0.12807 0.15769]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00363 0.01071 0.02157 0.03565 0.05319 0.07371 0.09698 0.12299]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00351 0.01043 0.02072 0.03438 0.0512  0.07082 0.09332]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00355 0.01004 0.01999 0.03309 0.04918 0.06816]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.0035  0.00973 0.01932 0.03181 0.04736]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00338 0.00955 0.01854 0.03075]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00333 0.00909 0.01793]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.00324 0.00891]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.0031 ]
 [0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.      0.     ]]
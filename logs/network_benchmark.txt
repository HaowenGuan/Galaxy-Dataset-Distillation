C:\Users\EXCAE\PycharmProjects\Galaxy-Dataset-Distillation\venv\Scripts\python.exe C:/Users/EXCAE/PycharmProjects/Galaxy-Dataset-Distillation/model_benchmark.py
100%|██████████| 8050/8050 [00:00<00:00, 473105.88it/s]
BUILDING training DATASET
100%|██████████| 2013/2013 [00:00<00:00, 335217.93it/s]
BUILDING testing DATASET
real images channel 0, mean = -0.0012, std = 0.9841
real images channel 1, mean = -0.0003, std = 0.9838
real images channel 2, mean = 0.0001, std = 0.9832
Benchmarking galaxy -------------------------------------------------------------------
  0%|          | 0/32 [00:00<?, ?it/s]C:\Users\EXCAE\PycharmProjects\Galaxy-Dataset-Distillation\venv\lib\site-packages\torch\nn\modules\conv.py:460: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\Convolution.cpp:894.)
  self.padding, self.dilation, self.groups)
100%|██████████| 32/32 [00:02<00:00, 10.86it/s]
100%|██████████| 8/8 [00:00<00:00, 43.92it/s]
Training loss at epoch 0: 10.225873735321235, test loss: 9.596301954774434
100%|██████████| 32/32 [00:01<00:00, 18.57it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 1: 9.44642673658288, test loss: 9.004667233781204
100%|██████████| 32/32 [00:01<00:00, 18.54it/s]
100%|██████████| 8/8 [00:00<00:00, 49.04it/s]
Training loss at epoch 2: 8.927305779309005, test loss: 8.634978414233265
100%|██████████| 32/32 [00:01<00:00, 18.52it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 3: 8.605831693092489, test loss: 8.398841854120095
100%|██████████| 32/32 [00:01<00:00, 18.51it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 4: 8.413195414098894, test loss: 8.280128566175328
100%|██████████| 32/32 [00:01<00:00, 18.56it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 5: 8.312141901809976, test loss: 8.215395097879409
100%|██████████| 32/32 [00:01<00:00, 18.59it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 6: 8.262566630677407, test loss: 8.186927185210196
100%|██████████| 32/32 [00:01<00:00, 18.56it/s]
100%|██████████| 8/8 [00:00<00:00, 48.44it/s]
Training loss at epoch 7: 8.231981966954582, test loss: 8.178252006489071
100%|██████████| 32/32 [00:01<00:00, 18.62it/s]
100%|██████████| 8/8 [00:00<00:00, 47.86it/s]
Training loss at epoch 8: 8.223508080903047, test loss: 8.180179783555328
100%|██████████| 32/32 [00:01<00:00, 18.61it/s]
100%|██████████| 8/8 [00:00<00:00, 48.44it/s]
Training loss at epoch 9: 8.221895115064537, test loss: 8.185399396112768
100%|██████████| 32/32 [00:01<00:00, 18.29it/s]
100%|██████████| 8/8 [00:00<00:00, 47.86it/s]
Training loss at epoch 10: 8.222376974354619, test loss: 8.176877517813898
100%|██████████| 32/32 [00:01<00:00, 17.70it/s]
100%|██████████| 8/8 [00:00<00:00, 46.20it/s]
Training loss at epoch 11: 8.21800514363354, test loss: 8.190183242517387
100%|██████████| 32/32 [00:01<00:00, 17.95it/s]
100%|██████████| 8/8 [00:00<00:00, 46.47it/s]
Training loss at epoch 12: 8.217068674786491, test loss: 8.178097857092958
100%|██████████| 32/32 [00:01<00:00, 17.89it/s]
100%|██████████| 8/8 [00:00<00:00, 44.16it/s]
Training loss at epoch 13: 8.219095936650815, test loss: 8.186252069070573
100%|██████████| 32/32 [00:01<00:00, 17.80it/s]
100%|██████████| 8/8 [00:00<00:00, 36.00it/s]
Training loss at epoch 14: 8.220223441745924, test loss: 8.179222019762172
100%|██████████| 32/32 [00:01<00:00, 17.85it/s]
100%|██████████| 8/8 [00:00<00:00, 32.10it/s]
Training loss at epoch 15: 8.222147095484763, test loss: 8.180002044814177
100%|██████████| 32/32 [00:01<00:00, 18.15it/s]
100%|██████████| 8/8 [00:00<00:00, 47.86it/s]
Training loss at epoch 16: 8.215965568589867, test loss: 8.184031759869443
100%|██████████| 32/32 [00:01<00:00, 17.77it/s]
100%|██████████| 8/8 [00:00<00:00, 45.67it/s]
Training loss at epoch 17: 8.219466620972439, test loss: 8.177911567972243
100%|██████████| 32/32 [00:01<00:00, 17.79it/s]
100%|██████████| 8/8 [00:00<00:00, 46.74it/s]
Training loss at epoch 18: 8.218211366641595, test loss: 8.183460157899434
100%|██████████| 32/32 [00:01<00:00, 17.51it/s]
100%|██████████| 8/8 [00:00<00:00, 45.94it/s]
Training loss at epoch 19: 8.221101794509414, test loss: 8.18137695797628
100%|██████████| 32/32 [00:01<00:00, 17.88it/s]
100%|██████████| 8/8 [00:00<00:00, 47.29it/s]
Training loss at epoch 20: 8.21717041015625, test loss: 8.17798476164695
100%|██████████| 32/32 [00:01<00:00, 18.28it/s]
100%|██████████| 8/8 [00:00<00:00, 47.29it/s]
Training loss at epoch 21: 8.220897724791342, test loss: 8.179603087742176
100%|██████████| 32/32 [00:01<00:00, 18.47it/s]
100%|██████████| 8/8 [00:00<00:00, 46.74it/s]
Training loss at epoch 22: 8.21584261118255, test loss: 8.181599389126925
100%|██████████| 32/32 [00:01<00:00, 18.53it/s]
100%|██████████| 8/8 [00:00<00:00, 47.86it/s]
Training loss at epoch 23: 8.221718219259511, test loss: 8.180523132839667
100%|██████████| 32/32 [00:01<00:00, 18.49it/s]
100%|██████████| 8/8 [00:00<00:00, 48.15it/s]
Training loss at epoch 24: 8.221496020962732, test loss: 8.180709118755434
100%|██████████| 32/32 [00:01<00:00, 18.42it/s]
100%|██████████| 8/8 [00:00<00:00, 46.74it/s]
Training loss at epoch 25: 8.22207889533191, test loss: 8.18383819382995
100%|██████████| 32/32 [00:01<00:00, 18.38it/s]
100%|██████████| 8/8 [00:00<00:00, 46.74it/s]
Training loss at epoch 26: 8.22088451693517, test loss: 8.181389025533253
100%|██████████| 32/32 [00:01<00:00, 18.35it/s]
100%|██████████| 8/8 [00:00<00:00, 48.15it/s]
Training loss at epoch 27: 8.217608544012034, test loss: 8.186957202500155
100%|██████████| 32/32 [00:01<00:00, 18.37it/s]
100%|██████████| 8/8 [00:00<00:00, 47.58it/s]
Training loss at epoch 28: 8.221161586216517, test loss: 8.177070174238544
100%|██████████| 32/32 [00:01<00:00, 18.34it/s]
100%|██████████| 8/8 [00:00<00:00, 47.86it/s]
Training loss at epoch 29: 8.22076385687597, test loss: 8.178274807501243
Benchmarking ConvNet -------------------------------------------------------------------
100%|██████████| 32/32 [00:01<00:00, 18.11it/s]
100%|██████████| 8/8 [00:00<00:00, 49.95it/s]
Training loss at epoch 0: 44.09346254336908, test loss: 6.623583062631955
100%|██████████| 32/32 [00:01<00:00, 18.48it/s]
100%|██████████| 8/8 [00:00<00:00, 49.95it/s]
Training loss at epoch 1: 3.6711471652392276, test loss: 2.183282222075067
100%|██████████| 32/32 [00:01<00:00, 18.51it/s]
100%|██████████| 8/8 [00:00<00:00, 49.34it/s]
Training loss at epoch 2: 1.796330475422166, test loss: 1.5827184336965971
100%|██████████| 32/32 [00:01<00:00, 18.51it/s]
100%|██████████| 8/8 [00:00<00:00, 49.64it/s]
Training loss at epoch 3: 1.487253725040033, test loss: 1.5010052608484306
100%|██████████| 32/32 [00:01<00:00, 18.40it/s]
100%|██████████| 8/8 [00:00<00:00, 48.74it/s]
Training loss at epoch 4: 1.4269210256256672, test loss: 1.3051457942805358
100%|██████████| 32/32 [00:01<00:00, 18.52it/s]
100%|██████████| 8/8 [00:00<00:00, 50.27it/s]
Training loss at epoch 5: 1.3456329213018003, test loss: 1.3972248621266457
100%|██████████| 32/32 [00:01<00:00, 18.49it/s]
100%|██████████| 8/8 [00:00<00:00, 49.04it/s]
Training loss at epoch 6: 1.357993074973918, test loss: 1.7882590220451828
100%|██████████| 32/32 [00:01<00:00, 18.67it/s]
100%|██████████| 8/8 [00:00<00:00, 49.03it/s]
Training loss at epoch 7: 1.2966334675854037, test loss: 1.3014166614515414
100%|██████████| 32/32 [00:01<00:00, 18.52it/s]
100%|██████████| 8/8 [00:00<00:00, 49.34it/s]
Training loss at epoch 8: 1.1927538512804494, test loss: 1.2778702502338317
100%|██████████| 32/32 [00:01<00:00, 18.54it/s]
100%|██████████| 8/8 [00:00<00:00, 49.06it/s]
Training loss at epoch 9: 1.1197217213885384, test loss: 1.0345748241896424

Process finished with exit code 0
